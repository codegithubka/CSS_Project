{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dc2fea",
   "metadata": {},
   "source": [
    "# Phase 3: Finite-Size Scaling (FSS) Analysis\n",
    "\n",
    "This notebook analyzes Phase 3 experiment results which test **finite-size scaling** at the critical point.\n",
    "\n",
    "## Key Questions\n",
    "- How does the cluster size distribution change with system size L?\n",
    "- Does the maximum cluster size scale as $s_{max} \\sim L^D$ (fractal dimension)?\n",
    "- Does the largest cluster fraction show critical scaling?\n",
    "\n",
    "## Theory\n",
    "At criticality, we expect:\n",
    "- **Cluster size distribution**: $P(s) \\sim s^{-\\tau}$ with cutoff at $s_{max}$\n",
    "- **Maximum cluster size**: $s_{max} \\sim L^{D}$ where D is the fractal dimension\n",
    "- **Order parameter**: $P_\\infty \\sim L^{-\\beta/\\nu}$ (largest cluster fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Add project root to path and import analysis functions\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "# Add scripts directory to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"scripts\"))\n",
    "\n",
    "# Import analysis functions\n",
    "from analysis import load_phase3_results, plot_phase3_fss_analysis\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import json\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"✓ Analysis functions imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4570cd6",
   "metadata": {},
   "source": [
    "## Load Phase 3 Results\n",
    "\n",
    "Load the Phase 3 FSS experiment results and examine the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c94aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your Phase 3 results\n",
    "results_dir = project_root / \"hpc_data\" / \"phase3_18698382\"\n",
    "\n",
    "# Load the results\n",
    "results = load_phase3_results(results_dir)\n",
    "print(f\"✓ Loaded {len(results)} simulation results\")\n",
    "\n",
    "# Load metadata\n",
    "with open(results_dir / \"phase3_metadata.json\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nExperiment Details:\")\n",
    "print(f\"  Critical point: prey_birth={metadata['critical_point']['prey_birth']}, \"\n",
    "      f\"prey_death={metadata['critical_point']['prey_death']}\")\n",
    "print(f\"  Grid sizes: {metadata['grid_sizes']}\")\n",
    "print(f\"  Replicates per size: {metadata['n_sims'] // len(metadata['grid_sizes'])}\")\n",
    "\n",
    "# Quick peek at the data structure\n",
    "print(f\"\\nSample result keys: {list(results[0].keys())[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b00e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse cluster sizes\n",
    "def parse_clusters(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "# Extract data by grid size\n",
    "grid_sizes = sorted(set(r['grid_size'] for r in results))\n",
    "\n",
    "# Data structures for analysis\n",
    "data_by_L = {L: {\n",
    "    'prey_clusters': [],\n",
    "    'prey_largest_frac': [],\n",
    "    'prey_max_cluster': [],\n",
    "    'prey_n_clusters': [],\n",
    "    'prey_survived': [],\n",
    "    'pred_survived': [],\n",
    "} for L in grid_sizes}\n",
    "\n",
    "# Collect data\n",
    "for r in results:\n",
    "    L = r['grid_size']\n",
    "    prey_clusters = parse_clusters(r.get('prey_cluster_sizes', []))\n",
    "    \n",
    "    data_by_L[L]['prey_clusters'].extend(prey_clusters)\n",
    "    data_by_L[L]['prey_survived'].append(r.get('prey_survived', False))\n",
    "    data_by_L[L]['pred_survived'].append(r.get('pred_survived', False))\n",
    "    \n",
    "    if r.get('prey_largest_fraction') is not None and not np.isnan(r.get('prey_largest_fraction', np.nan)):\n",
    "        data_by_L[L]['prey_largest_frac'].append(r['prey_largest_fraction'])\n",
    "    \n",
    "    if prey_clusters:\n",
    "        data_by_L[L]['prey_max_cluster'].append(max(prey_clusters))\n",
    "        data_by_L[L]['prey_n_clusters'].append(len(prey_clusters))\n",
    "\n",
    "# Summary by grid size\n",
    "print(\"Data Summary by Grid Size:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'L':<8} {'n_runs':<10} {'prey_surv':<12} {'pred_surv':<12} {'s_max (mean)':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for L in grid_sizes:\n",
    "    n_runs = len([r for r in results if r['grid_size'] == L])\n",
    "    prey_surv = np.mean(data_by_L[L]['prey_survived']) * 100\n",
    "    pred_surv = np.mean(data_by_L[L]['pred_survived']) * 100\n",
    "    s_max = np.mean(data_by_L[L]['prey_max_cluster']) if data_by_L[L]['prey_max_cluster'] else 0\n",
    "    print(f\"{L:<8} {n_runs:<10} {prey_surv:<12.0f}% {pred_surv:<12.0f}% {s_max:<15.1f}\")\n",
    "print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d0c3b",
   "metadata": {},
   "source": [
    "## Finite-Size Scaling Plots\n",
    "\n",
    "Generate the 4-panel FSS analysis figure:\n",
    "1. **Cluster Size Distributions**: P(s) for each grid size L\n",
    "2. **Maximum Cluster Scaling**: $s_{max}$ vs L (extract fractal dimension D)\n",
    "3. **Order Parameter Scaling**: Largest cluster fraction vs L\n",
    "4. **Cluster Count Scaling**: Number of clusters vs L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 4-panel FSS analysis figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "fig.suptitle(\"Phase 3: Finite-Size Scaling at Critical Point\", fontsize=14, fontweight='bold')\n",
    "\n",
    "# Color map for different grid sizes\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(grid_sizes)))\n",
    "\n",
    "# =============================================================================\n",
    "# Panel 1: Cluster Size Distributions by Grid Size\n",
    "# =============================================================================\n",
    "ax = axes[0, 0]\n",
    "\n",
    "for L, color in zip(grid_sizes, colors):\n",
    "    clusters = np.array(data_by_L[L]['prey_clusters'])\n",
    "    if len(clusters) == 0:\n",
    "        continue\n",
    "    clusters = clusters[clusters > 0]\n",
    "    \n",
    "    # Compute histogram\n",
    "    sizes, counts = np.unique(clusters, return_counts=True)\n",
    "    # Normalize\n",
    "    n_replicates = len([r for r in results if r['grid_size'] == L])\n",
    "    counts = counts / n_replicates\n",
    "    \n",
    "    ax.scatter(sizes, counts, alpha=0.5, s=15, color=color, label=f'L={L}')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Cluster Size s', fontsize=12)\n",
    "ax.set_ylabel('P(s) (normalized)', fontsize=12)\n",
    "ax.set_title('Cluster Size Distribution vs System Size', fontsize=13)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Panel 2: Largest Cluster Fraction vs L\n",
    "# =============================================================================\n",
    "ax = axes[0, 1]\n",
    "\n",
    "L_vals_frac = []\n",
    "frac_mean = []\n",
    "frac_std = []\n",
    "\n",
    "for L in grid_sizes:\n",
    "    fracs = data_by_L[L]['prey_largest_frac']\n",
    "    if len(fracs) > 0:\n",
    "        L_vals_frac.append(L)\n",
    "        frac_mean.append(np.mean(fracs))\n",
    "        frac_std.append(np.std(fracs))\n",
    "\n",
    "L_vals_frac = np.array(L_vals_frac)\n",
    "frac_mean = np.array(frac_mean)\n",
    "frac_std = np.array(frac_std)\n",
    "\n",
    "ax.errorbar(L_vals_frac, frac_mean, yerr=frac_std, fmt='s-', color='forestgreen',\n",
    "            markersize=8, capsize=5, linewidth=2, label='Data')\n",
    "\n",
    "# Fit power law\n",
    "if len(L_vals_frac) >= 3:\n",
    "    log_L = np.log10(L_vals_frac)\n",
    "    log_frac = np.log10(frac_mean)\n",
    "    slope_frac, intercept_frac, r_value_frac, _, _ = linregress(log_L, log_frac)\n",
    "    \n",
    "    fit_L = np.logspace(np.log10(L_vals_frac.min()), np.log10(L_vals_frac.max()), 50)\n",
    "    fit_frac = 10**intercept_frac * fit_L**slope_frac\n",
    "    ax.plot(fit_L, fit_frac, 'r--', linewidth=2,\n",
    "            label=f'Fit: $P_{{\\\\infty}} \\\\sim L^{{{slope_frac:.2f}}}$ (R²={r_value_frac**2:.3f})')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('System Size L', fontsize=12)\n",
    "ax.set_ylabel('Largest Cluster Fraction $P_\\\\infty$', fontsize=12)\n",
    "ax.set_title('Order Parameter Scaling', fontsize=13)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29704760",
   "metadata": {},
   "source": [
    "## Power-Law Fitting with p-values\n",
    "\n",
    "Use the `powerlaw` package to rigorously test if cluster size distributions follow a power law.\n",
    "For each grid size L, we:\n",
    "1. Fit a power law to the cluster size distribution\n",
    "2. Compare against alternative distributions (exponential, log-normal)\n",
    "3. Extract p-values to assess goodness of fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Store results for each grid size\n",
    "powerlaw_results = []\n",
    "\n",
    "print(\"Power-Law Fitting Results by Grid Size\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"{'L':<8} {'α (tau)':<10} {'x_min':<10} {'KS stat':<10} {'PL vs StrExp':<18} {'PL vs LN':<18} {'n_tail':<10}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for L in grid_sizes:\n",
    "    clusters = np.array(data_by_L[L]['prey_clusters'])\n",
    "    clusters = clusters[clusters > 0]  # Remove zeros\n",
    "    \n",
    "    if len(clusters) < 50:\n",
    "        print(f\"{L:<8} Insufficient data ({len(clusters)} points)\")\n",
    "        continue\n",
    "    \n",
    "    # Fit power law\n",
    "    fit = powerlaw.Fit(clusters, discrete=True, verbose=False)\n",
    "    \n",
    "    # Get KS statistic (D) - this is the distance, not a method call\n",
    "    ks_stat = fit.power_law.D\n",
    "    \n",
    "    # Compare power law vs alternatives\n",
    "    # Positive R means power law is better, negative means alternative is better\n",
    "    # p tells us if the difference is significant\n",
    "    \n",
    "    # vs Stretched Exponential (Weibull)\n",
    "    R_strexp, p_strexp = fit.distribution_compare('power_law', 'stretched_exponential', normalized_ratio=True)\n",
    "    \n",
    "    # vs Lognormal\n",
    "    R_ln, p_ln = fit.distribution_compare('power_law', 'lognormal', normalized_ratio=True)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'L': L,\n",
    "        'alpha': fit.power_law.alpha,\n",
    "        'xmin': fit.power_law.xmin,\n",
    "        'ks_stat': ks_stat,\n",
    "        'n_tail': int(np.sum(clusters >= fit.power_law.xmin)),\n",
    "        'R_vs_strexp': R_strexp,\n",
    "        'p_vs_strexp': p_strexp,\n",
    "        'R_vs_ln': R_ln,\n",
    "        'p_vs_ln': p_ln,\n",
    "        'n_total': len(clusters)\n",
    "    }\n",
    "    powerlaw_results.append(result)\n",
    "    \n",
    "    # Format comparison results\n",
    "    strexp_result = f\"R={R_strexp:+.2f} (p={p_strexp:.3f})\"\n",
    "    ln_result = f\"R={R_ln:+.2f} (p={p_ln:.3f})\"\n",
    "    \n",
    "    print(f\"{L:<8} {fit.power_law.alpha:<10.3f} {fit.power_law.xmin:<10.1f} {ks_stat:<10.4f} \"\n",
    "          f\"{strexp_result:<18} {ln_result:<18} {result['n_tail']:<10}\")\n",
    "\n",
    "print(\"-\" * 110)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • α (tau): Power-law exponent P(s) ~ s^(-α)\")\n",
    "print(\"  • x_min: Lower bound where power law holds\")\n",
    "print(\"  • KS stat: Kolmogorov-Smirnov distance (lower = better fit)\")\n",
    "print(\"  • StrExp: Stretched exponential (Weibull) - common alternative to power law\")\n",
    "print(\"  • LN: Lognormal - often confused with power law in empirical data\")\n",
    "print(\"  • R>0 + p<0.05: Power law is SIGNIFICANTLY better than alternative\")\n",
    "print(\"  • R<0 + p<0.05: Alternative is SIGNIFICANTLY better than power law\")\n",
    "print(\"  • p>0.05: No significant difference between distributions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize power-law fitting results vs grid size\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Power-Law Fitting Analysis vs System Size\", fontsize=14, fontweight='bold')\n",
    "\n",
    "if powerlaw_results:\n",
    "    L_arr = np.array([r['L'] for r in powerlaw_results])\n",
    "    alpha_arr = np.array([r['alpha'] for r in powerlaw_results])\n",
    "    ks_arr = np.array([r['ks_stat'] for r in powerlaw_results])\n",
    "    xmin_arr = np.array([r['xmin'] for r in powerlaw_results])\n",
    "    R_strexp_arr = np.array([r['R_vs_strexp'] for r in powerlaw_results])\n",
    "    R_ln_arr = np.array([r['R_vs_ln'] for r in powerlaw_results])\n",
    "\n",
    "    # Panel 1: Power-law exponent α vs L\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(L_arr, alpha_arr, 'o-', color='steelblue', markersize=10, linewidth=2)\n",
    "    ax.axhline(2.0, color='red', linestyle='--', label='α = 2 (mean field)')\n",
    "    ax.axhline(2.05, color='green', linestyle=':', label='α ≈ 2.05 (2D percolation)')\n",
    "    ax.set_xlabel('Grid Size L', fontsize=12)\n",
    "    ax.set_ylabel('Power-law exponent α', fontsize=12)\n",
    "    ax.set_title('Power-Law Exponent vs System Size', fontsize=13)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 2: KS statistic vs L (lower = better fit)\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(L_arr, ks_arr, 's-', color='darkgreen', markersize=10, linewidth=2)\n",
    "    ax.set_xlabel('Grid Size L', fontsize=12)\n",
    "    ax.set_ylabel('KS Statistic D (lower = better)', fontsize=12)\n",
    "    ax.set_title('Power-Law Fit Quality vs System Size', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 3: x_min vs L\n",
    "    ax = axes[1, 0]\n",
    "    ax.loglog(L_arr, xmin_arr, 'd-', color='purple', markersize=10, linewidth=2)\n",
    "    ax.set_xlabel('Grid Size L', fontsize=12)\n",
    "    ax.set_ylabel('$x_{min}$ (power-law cutoff)', fontsize=12)\n",
    "    ax.set_title('Lower Cutoff vs System Size', fontsize=13)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Panel 4: Likelihood ratio comparisons (Stretched Exp & Lognormal)\n",
    "    ax = axes[1, 1]\n",
    "    x = np.arange(len(L_arr))\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, R_strexp_arr, width, label='vs Stretched Exp', color='coral')\n",
    "    ax.bar(x + width/2, R_ln_arr, width, label='vs Log-normal', color='skyblue')\n",
    "    ax.axhline(0, color='black', linewidth=1)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'L={L}' for L in L_arr])\n",
    "    ax.set_xlabel('Grid Size', fontsize=12)\n",
    "    ax.set_ylabel('Log-likelihood ratio R', fontsize=12)\n",
    "    ax.set_title('Power Law vs Alternatives (R>0 favors power law)', fontsize=13)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot individual power-law fits with lognormal comparison for each grid size\n",
    "n_sizes = len(grid_sizes)\n",
    "n_cols = min(3, n_sizes)\n",
    "n_rows = (n_sizes + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "if n_sizes == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "    \n",
    "fig.suptitle(\"Power-Law vs Lognormal Fits by Grid Size\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, L in enumerate(grid_sizes):\n",
    "    row, col = idx // n_cols, idx % n_cols\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    clusters = np.array(data_by_L[L]['prey_clusters'])\n",
    "    clusters = clusters[clusters > 0]\n",
    "    \n",
    "    if len(clusters) < 50:\n",
    "        ax.text(0.5, 0.5, f'L={L}\\nInsufficient data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    \n",
    "    # Fit power law\n",
    "    fit = powerlaw.Fit(clusters, discrete=True, verbose=False)\n",
    "    \n",
    "    # Get result from our stored results\n",
    "    result = next((r for r in powerlaw_results if r['L'] == L), None)\n",
    "    \n",
    "    # Plot CCDF (complementary cumulative distribution)\n",
    "    fit.plot_ccdf(ax=ax, color='steelblue', linewidth=0, marker='o', markersize=4, alpha=0.6, label='Data')\n",
    "    fit.power_law.plot_ccdf(ax=ax, color='red', linestyle='--', linewidth=2, label=f'Power law (α={fit.power_law.alpha:.2f})')\n",
    "    \n",
    "    # Also plot lognormal fit for comparison\n",
    "    fit.lognormal.plot_ccdf(ax=ax, color='green', linestyle=':', linewidth=2, label='Lognormal')\n",
    "    \n",
    "    # Add fit info with R values for both comparisons\n",
    "    R_ln = result['R_vs_ln'] if result else np.nan\n",
    "    R_strexp = result['R_vs_strexp'] if result else np.nan\n",
    "    ax.set_title(f'L={L} | R_ln={R_ln:+.2f} | R_strexp={R_strexp:+.2f}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Cluster Size s')\n",
    "    ax.set_ylabel('P(X ≥ s)')\n",
    "    ax.legend(fontsize=7, loc='lower left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Color code based on R vs lognormal (the more challenging comparison)\n",
    "    # R > 0 means power law is favored over lognormal\n",
    "    if result and R_ln > 0:\n",
    "        ax.set_facecolor('#e6ffe6')  # Light green - power law favored\n",
    "    elif result and R_ln > -1:\n",
    "        ax.set_facecolor('#fff3e6')  # Light orange - marginal\n",
    "    else:\n",
    "        ax.set_facecolor('#ffe6e6')  # Light red - lognormal favored\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_sizes, n_rows * n_cols):\n",
    "    row, col = idx // n_cols, idx % n_cols\n",
    "    axes[row, col].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nColor coding based on R vs Lognormal: Green = R>0 (power law), Orange = -1<R≤0, Red = R≤-1 (lognormal)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: Does power-law fit improve with grid size?\n",
    "print(\"=\" * 85)\n",
    "print(\"POWER-LAW FIT QUALITY vs GRID SIZE\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "if powerlaw_results:\n",
    "    # Sort by L\n",
    "    sorted_results = sorted(powerlaw_results, key=lambda x: x['L'])\n",
    "    \n",
    "    print(f\"\\n{'L':<8} {'α':<8} {'KS':<10} {'R_strexp':<12} {'R_ln':<12} {'Verdict (vs LN)':<25}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for r in sorted_results:\n",
    "        # Use R vs lognormal to judge (more stringent test)\n",
    "        R_ln = r['R_vs_ln']\n",
    "        p_ln = r['p_vs_ln']\n",
    "        R_strexp = r['R_vs_strexp']\n",
    "        \n",
    "        if R_ln > 0 and p_ln < 0.05:\n",
    "            verdict = \"✅ Power law (significant)\"\n",
    "        elif R_ln > 0:\n",
    "            verdict = \"✅ Power law (weak)\"\n",
    "        elif R_ln < 0 and p_ln < 0.05:\n",
    "            verdict = \"❌ Lognormal (significant)\"\n",
    "        else:\n",
    "            verdict = \"⚠️  Inconclusive\"\n",
    "        \n",
    "        print(f\"{r['L']:<8} {r['alpha']:<8.3f} {r['ks_stat']:<10.4f} {R_strexp:<+12.2f} {R_ln:<+12.2f} {verdict}\")\n",
    "    \n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    # Check if KS stat improves (decreases) with L\n",
    "    L_vals = [r['L'] for r in sorted_results]\n",
    "    ks_vals = [r['ks_stat'] for r in sorted_results]\n",
    "    R_ln_vals = [r['R_vs_ln'] for r in sorted_results]\n",
    "    R_strexp_vals = [r['R_vs_strexp'] for r in sorted_results]\n",
    "    \n",
    "    if len(L_vals) >= 3:\n",
    "        from scipy.stats import spearmanr\n",
    "        corr_ks, p_ks = spearmanr(L_vals, ks_vals)\n",
    "        corr_R_ln, p_R_ln = spearmanr(L_vals, R_ln_vals)\n",
    "        corr_R_strexp, p_R_strexp = spearmanr(L_vals, R_strexp_vals)\n",
    "        \n",
    "        print(f\"\\nCorrelations with grid size L:\")\n",
    "        print(f\"  KS stat:        r = {corr_ks:+.3f} (p = {p_ks:.3f})\")\n",
    "        print(f\"  R vs Lognormal: r = {corr_R_ln:+.3f} (p = {p_R_ln:.3f})\")\n",
    "        print(f\"  R vs StretchExp: r = {corr_R_strexp:+.3f} (p = {p_R_strexp:.3f})\")\n",
    "        \n",
    "        if corr_ks < -0.5 and p_ks < 0.1:\n",
    "            print(\"\\n→ KS stat DECREASES with L (fit improves)\")\n",
    "        elif corr_ks > 0.5 and p_ks < 0.1:\n",
    "            print(\"\\n→ KS stat INCREASES with L (fit worsens)\")\n",
    "        \n",
    "        if corr_R_ln > 0.5 and p_R_ln < 0.1:\n",
    "            print(\"→ Power law becomes MORE favored over lognormal at larger L\")\n",
    "        elif corr_R_ln < -0.5 and p_R_ln < 0.1:\n",
    "            print(\"→ Lognormal becomes MORE favored over power law at larger L\")\n",
    "    \n",
    "    # Mean exponent\n",
    "    alpha_mean = np.mean([r['alpha'] for r in sorted_results])\n",
    "    alpha_std = np.std([r['alpha'] for r in sorted_results])\n",
    "    print(f\"\\nMean power-law exponent: α = {alpha_mean:.3f} ± {alpha_std:.3f}\")\n",
    "    print(f\"  (2D percolation: τ ≈ 2.05, Mean field: τ = 2.0)\")\n",
    "\n",
    "print(\"=\" * 85)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clean_env)",
   "language": "python",
   "name": "clean_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
